
import uuid
from datasets import Dataset as HuggingFaceDataset, concatenate_datasets

class MedHal:

    """
    The MedHal dataset is a dataset of medical statements and their explanations.
    It is used to train and evaluate medical statement verification models.

    It contains the following columns:
    - id: The id of the sample.
    - context: The context of the statement.
    - statement: The statement to verify.
    - label: The label of the statement (True if factual, False otherwise).
    - explanation: The explanation of the statement.
    - inner_id: The id of the sample in the original dataset.
    - source: Dataset from which the sample comes from.
    - synthetic: Whether the sample is synthetic or not (was generated by a LLM).
    """

    @staticmethod
    def from_splitted_sumpubmed(positive_path: str, negative_path: str, output_path: str, llm_output_column: str = 'OUTPUT'):
        """
        Constructs the MedHal dataset from a splitted SumPubMed dataset. The dataset is splitted in the sense
        that positive and negative samples are in different files.

        Args:
            positive_path: Path to the positive samples.
            negative_path: Path to the negative samples.
            output_path: Path to the output file where the MedHal samples will be saved.
            llm_output_column: Column name of the LLM output in the negative samples.
        """
        positive_dataset = HuggingFaceDataset.from_csv(positive_path)
        negative_dataset = HuggingFaceDataset.from_csv(negative_path)

        def generate_positive_samples(x):

            unique_ids = [str(uuid.uuid4()) for _ in range(len(x['text']))]

            return {
                'id': unique_ids,
                'context': x['text'],
                'statement': x['summary'],
                'label': [True] * len(x['text']),
                'explanation': ['The statement is factual.'] * len(x['text']),
                'inner_id': x['id'],
                'source': ['sumpubmed'] * len(x['text']),
                'synthetic': [False] * len(x['text'])
            }

        positive_ready_dataset = positive_dataset.map(
            generate_positive_samples, 
            remove_columns=positive_dataset.column_names,
            batched=True,
            desc="Generating positive samples"
        )

        def generate_negative_samples(x):
            unique_ids = [str(uuid.uuid4()) for _ in range(len(x['text']))]

            fake_summaries = []
            for before, output, after in zip(x['before'], x[llm_output_column], x['after']):
                fake_summaries.append(f"{before} {output.lower()} {after}".strip())

            explanations = []
            for explanation in x['sentence']:
                explanations.append(f"According to the source document, {explanation}")

            return {
                'id': unique_ids,
                'context': x['text'],
                'statement': fake_summaries,
                'label': [False] * len(x['text']),
                'explanation': explanations,
                'inner_id': x['id'],
                'source': ['sumpubmed'] * len(x['text']),
                'synthetic': [True] * len(x['text'])
            }
        
        print('Before : ', len(negative_dataset))


        negative_dataset = negative_dataset.filter(
            lambda x: 'Here' not in x[llm_output_column] or 'transformed sentence' not in x[llm_output_column],
            desc="Filtering negative samples"
        )

        print(len(negative_dataset))

        negative_ready_dataset = negative_dataset.map(
            generate_negative_samples,
            remove_columns=negative_dataset.column_names,
            batched=True,
            desc="Generating negative samples"
        )

        # Merge the positive and negative datasets
        merged_dataset = concatenate_datasets([positive_ready_dataset, negative_ready_dataset])

        # Save the merged dataset
        merged_dataset = merged_dataset.shuffle(seed=42)
        if output_path is not None:
            merged_dataset.to_csv(output_path, index=False)

        return merged_dataset
