
from collections import defaultdict
import re
from typing import Dict, Tuple
import uuid
import nltk
import nltk.translate.bleu_score
from datasets import Dataset as HuggingFaceDataset, concatenate_datasets
import logging

from tqdm import tqdm

from src.data.med_hal.augmented_clinical_notes import AugmentedClinicalNotes


logger = logging.getLogger(__name__)

class MedHal:

    """
    The MedHal dataset is a dataset of medical statements and their explanations.
    It is used to train and evaluate medical statement verification models.

    It contains the following columns:
    - id: The id of the sample.
    - context: The context of the statement.
    - statement: The statement to verify.
    - label: The label of the statement (True if factual, False otherwise).
    - explanation: The explanation of the statement.
    - inner_id: The id of the sample in the original dataset.
    - source: Dataset from which the sample comes from.
    - synthetic: Whether the sample is synthetic or not (was generated by a LLM).
    """

    FACTUAL_EXPLANATION = "The statement is factual."

    @staticmethod
    def filter_multiple_sentence_statements(x):
        if x['statement'] == 'None':
            return False

        sentences = nltk.sent_tokenize(x['statement'])
        return len(sentences) == 1

    @staticmethod
    def from_all(
        acm_path: str,
        medmcqa_path: str,
        medqa_path: str,
        mednli_path: str,
        sumpubmed_positive_path: str,
        sumpubmed_negative_path: str,
        output_path: str = None
    ) -> HuggingFaceDataset:

        logger.info(f"Processing ACM dataset from {acm_path}")
        acm_dataset = MedHal.from_augmented_clinical_notes(acm_path)
        logger.info(f"Processing MedMCQA dataset from {medmcqa_path}")
        medmcqa_dataset = MedHal.from_medmcqa(medmcqa_path)
        logger.info(f"Processing MedQA dataset from {medqa_path}")
        medqa_dataset = MedHal.from_medqa(medqa_path)
        logger.info(f"Processing MedNLI dataset from {mednli_path}")
        mednli_dataset = MedHal.from_mednli(mednli_path)
        logger.info(f"Processing Splitted SumPubMed dataset from {sumpubmed_positive_path} and {sumpubmed_negative_path}")
        splitted_sumpubmed_dataset = MedHal.from_splitted_sumpubmed(sumpubmed_positive_path, sumpubmed_negative_path)

        assert set(acm_dataset.column_names) == set(medmcqa_dataset.column_names) == set(medqa_dataset.column_names) == set(splitted_sumpubmed_dataset.column_names), \
            f"All datasets must have the same column names {acm_dataset.column_names} vs {medmcqa_dataset.column_names} vs {medqa_dataset.column_names} vs {splitted_sumpubmed_dataset.column_names}"

        final_dataset = concatenate_datasets([acm_dataset, medmcqa_dataset, medqa_dataset, splitted_sumpubmed_dataset, mednli_dataset]).shuffle(seed=42)

        if output_path is not None:
            final_dataset.to_csv(output_path, index=False)

        return final_dataset
        
    @staticmethod
    def from_mednli(path: str, output_path: str = None):
        """
        Constructs the MedHal dataset from a processed MedNLI dataset.

        Args:
            path: Path to the processed MedNLI dataset.
            output_path: Path to the output file where the MedHal samples will be saved.
        """
        dataset = HuggingFaceDataset.from_csv(path)

        def transform_mednli_to_medhal(x):

            assert x['factual'][0] and not x['factual'][1], "The first sample must be correct and the second one must be incorrect"
            assert x['context'][0] == x['context'][1], f"The first sample must have the same context as the second one ({x['context'][0]} vs {x['context'][1]})"

            return {
                'id': [str(uuid.uuid4()) for _ in range(len(x['internal_id']))],
                'context': x['context'],
                'statement': x['statement'],
                'label': x['factual'],
                'explanation': [MedHal.FACTUAL_EXPLANATION, x['statement'][0]],
                'inner_id': [str(id) for id in x['internal_id']],
                'source': ['mednli'] * len(x['internal_id']),
                'synthetic': [False] * len(x['internal_id'])
            }
        
        dataset = dataset.map(
            transform_mednli_to_medhal,
            desc="Transforming MedNLI to MedHal",
            batched=True,
            batch_size=2,
            remove_columns=dataset.column_names
        )

        if output_path is not None:
            dataset.to_csv(output_path, index=False)

        return dataset

    @staticmethod
    def augmented_clinical_notes_none_samples():
        return {
            'id': ['None', 'None'],
            'context': ['None', 'None'],
            'statement': ['None', 'None'],
            'label': [False, False],
            'explanation': ['None', 'None'],
            'inner_id': ['None', 'None'],
            'source': ['None', 'None'],
            'synthetic': [None, None]
        }
    
    @staticmethod
    def from_augmented_clinical_notes(path: str, output_path: str = None, llm_output_column: str = 'output'):
        """
        Constructs the MedHal dataset from a processed augmented clinical notes dataset.

        Args:
            path: Path to the processed augmented clinical notes dataset.
            output_path: Path to the output file where the MedHal samples will be saved.
            llm_output_column: Column name of the LLM output in the augmented clinical notes dataset.
        """
        dataset = HuggingFaceDataset.from_csv(path)

        def filter_dataset(hf_dataset: HuggingFaceDataset) -> HuggingFaceDataset:
            chosen_indices = [] # row indices we will keep
            label_seen = defaultdict(set) # (idx, key_path) -> {labels kept}

            for i in tqdm(range(len(hf_dataset)), total=len(hf_dataset)):
                key = (hf_dataset[i]["idx"], hf_dataset[i]["key_path"])
                label = bool(hf_dataset[i]["factual"])
                if label not in label_seen[key]:
                    # First time we see this label
                    chosen_indices.append(i)
                    label_seen[key].add(label)

            print('created label seens')
            # Only keep pairs that have both labels
            valid_keys = {k for k, labs in label_seen.items() if labs == {True, False}}
            final_indices = [i for i in chosen_indices
                            if (hf_dataset[i]["idx"], hf_dataset[i]["key_path"]) in valid_keys]
            print('sorting and selecting')

            return hf_dataset.select(sorted(final_indices))

        def acm_to_medhal(x):
            # x should contain an even number of samples where even indices are factual and odd indices are not factual
            assert len(x['concept']) % 2 == 0, 'Number of samples in batch must be even'
            results = defaultdict(list)
            results['id'].extend([str(uuid.uuid4()) for _ in range(len(x['idx']))])
            results['inner_id'].extend([str(id) for id in x['idx']])
            results['source'].extend(['acm'] * len(x['idx']))
            results['synthetic'].extend([True] * len(x['idx']))

            for i in range(len(x['concept']) // 2):
                factual_i = i * 2
                not_factual_i = factual_i + 1
                
                consecutive_samples = (x['factual'][factual_i] and not x['factual'][not_factual_i]) or (not x['factual'][factual_i] and x['factual'][not_factual_i])
                none_sample = not consecutive_samples or (x['idx'][factual_i] != x['idx'][not_factual_i]) or (x['key_path'][factual_i] != x['key_path'][not_factual_i])

                if none_sample:
                    results['context'].extend(['None', 'None'])
                    results['statement'].extend(['None', 'None'])
                    results['label'].extend([None, None])
                    results['explanation'].extend(['None', 'None'])
                    continue

                # if x['concept'][factual_i] == 'sex':
                    # factual_statement, hallucinated_statement = AugmentedClinicalNotes.fix_sex_generations(x[llm_output_column][factual_i], x[llm_output_column][not_factual_i])
                # else:
                factual_statement = x[llm_output_column][factual_i]
                hallucinated_statement = x[llm_output_column][not_factual_i]

                results['context'].extend([x['full_note'][factual_i], x['full_note'][not_factual_i]])
                results['statement'].extend([factual_statement, hallucinated_statement])
                results['label'].extend([True, False])
                results['explanation'].extend([MedHal.FACTUAL_EXPLANATION, factual_statement])
            return results

        dataset = dataset.filter(lambda x: [True if out is not None else False for out in x[llm_output_column]], batched=True)
        not_valid = dataset.filter(lambda x: [True if out is None else False for out in x[llm_output_column]], batched=True)
        dataset = filter_dataset(dataset)
        dataset = dataset.sort(['idx', 'key_path'])
        
        print('len : ', len(dataset))

        # valid_samples: Dict[Tuple[str, str, str], int] = {}
        # for row in dataset.iter(batch_size=1):

        #     key = (row['idx'], row['key_path'], row['factual'])
        #     if key in valid_samples:
        #         valid_samples[key] += -1 if row['factual']


        # dataset = HuggingFaceDataset.from_pandas(dataset.to_pandas().drop_duplicates(['idx', 'key_path', 'factual']))
        # print(dataset[0])
        # print('after dropping duplicates : ', len(dataset))
        
        # dataset = dataset.filter(
        #     lambda x: [True, True] if x[llm_output_column][0] != None and x[llm_output_column][1] != None else [False, False], 
        #     desc="Filtering None samples",
        #     batch_size=2,
        #     batched=True
        # )
        
        dataset = dataset.map(
            acm_to_medhal,
            desc="ACM to MedHal",
            batched=True,
            batch_size=1000,
            remove_columns=dataset.column_names
        )

        dataset = dataset.filter(lambda x: x['statement'] != 'None', desc="Filtering multiple sentence statements")

        if output_path is not None:
            dataset.to_csv(output_path, index=False)

        return dataset


    @staticmethod
    def from_medmcqa(path: str, output_path: str = None, llm_output_column: str = 'output'):
        """
        Constructs the MedHal dataset from a processed MedMCQA dataset.

        Args:
            path: Path to the processed MedMCQA dataset.
            output_path: Path to the output file where the MedHal samples will be saved.
            llm_output_column: Column name of the LLM output in the MedMCQA dataset.
        """
        dataset = HuggingFaceDataset.from_csv(path)
        

        def filter_dataset(hf_dataset: HuggingFaceDataset) -> HuggingFaceDataset:
            chosen_indices = [] # row indices we will keep
            label_seen = defaultdict(set) # (idx, key_path) -> {labels kept}

            for i in tqdm(range(len(hf_dataset)), total=len(hf_dataset)):
                key = hf_dataset[i]['id']
                label = bool(hf_dataset[i]['is_correct'])
                if label not in label_seen[key]:
                    # First time we see this label
                    chosen_indices.append(i)
                    label_seen[key].add(label)

            print('created label seens')
            # Only keep pairs that have both labels
            valid_keys = {k for k, labs in label_seen.items() if labs == {True, False}}
            final_indices = [i for i in chosen_indices
                            if hf_dataset[i]['id'] in valid_keys]
            print('sorting and selecting')

            return hf_dataset.select(sorted(final_indices))


        def transform_medmcqa_to_medhal(x):
            assert len(x['id']) % 2 == 0, 'Batch size must be even'

            final_dict = defaultdict(list)
            final_dict['id'] = [str(uuid.uuid4()) for _ in range(len(x['id']))]
            final_dict['inner_id'] = [str(id) for id in x['id']]
            final_dict['source'] = ['medmcqa'] * len(x['id'])
            final_dict['synthetic'] = [False] * len(x['id'])

            for i in range(len(x['id']) // 2):
                factual_i = i * 2
                not_factual_i = factual_i + 1

                # assert x['is_correct'][factual_i] and not x['is_correct'][not_factual_i], "The first sample must be correct and the second one must be incorrect"
                # assert x['id'][factual_i] == x['id'][not_factual_i], "The first sample must be before the second one"

                # Determine if context is needed. To do so, we verify if the question has more than one sentence
                # If it does, we remove the last sentence and use it as the context
                # Otherwise, we use 'None' as the context
                if len(nltk.sent_tokenize(x['question'][factual_i])) > 1:
                    context = ' '.join(nltk.sent_tokenize(x['question'][factual_i])[:-1])
                else:
                    context = None

                # return {
                    # 'id': [str(uuid.uuid4()) for _ in range(len(x['id']))],
                final_dict['context'].extend([context, context])
                final_dict['statement'].extend([
                    x[llm_output_column][factual_i], 
                    x[llm_output_column][not_factual_i]
                ])
                final_dict['label'].extend([
                    x['is_correct'][factual_i], 
                    x['is_correct'][not_factual_i]
                ])
                # Explanation of the factual statement is the explanation given in the dataset
                # Explanation of the non-factual statement is the true statement
                explanation_factual = x['explanation'][factual_i] if x['explanation'][factual_i] is not None and len(x['explanation'][factual_i]) > 0 else MedHal.FACTUAL_EXPLANATION
                final_dict['explanation'].extend([explanation_factual, x[llm_output_column][factual_i]])
                # }
            return final_dict
        
        dataset = dataset.filter(lambda x: x[llm_output_column] != None, desc="Filtering None samples")
        dataset = filter_dataset(dataset)

        dataset = dataset.map(
            transform_medmcqa_to_medhal, 
            desc="Transforming MedMCQA to MedHal",
            batched=True,
            batch_size=1000,
            remove_columns=dataset.column_names
        )

        dataset = dataset.filter(MedHal.filter_multiple_sentence_statements, desc="Filtering multiple sentence statements")
        dataset = MedHal.fix_medmcqa_explanations(dataset)

        if output_path is not None:
            dataset.to_csv(output_path, index=False)

        return dataset

    MEDMCQA_EXPLANATION_REGEX_PATTERN = """Ans(?:wer|wee)*(?:\.|:)* *(?:is)* *(?:\(|'|"|-)* *[?:a-d|A-D]* *(?:\(|\)|'|")* *(?:i\.e\.|:)*(?:\.|\,)*"""

    @staticmethod
    def fix_medmcqa_explanations(dataset: HuggingFaceDataset) -> HuggingFaceDataset:
        """
        In certain samples of the MedMCQA dataset, the explanation will contain the option of the sample (i.e. 'Ans: b (Benign HTN)'). We remove those as they don't make sense in this dataset

        Args: 
            dataset: HuggingFace dataset containing the samples (can be all samples or just MedMCQA)
        
        Returns the dataset where the explanation is fixed
        """
        pattern = re.compile(MedHal.MEDMCQA_EXPLANATION_REGEX_PATTERN, re.IGNORECASE)

        def fix_explanations(row):
            if row['explanation'] is None:
                return row
            try:
                new_explanation = re.sub(pattern, '', row['explanation'])
                return {'explanation': new_explanation}
            except:
                # If regex did not work, we simply remove the ans
                index_ans = row['explanation'].find('Ans')
                if index_ans > -1:
                    return {'explanation': row['explanation'][index_ans + len('Ans'):]}
                return row

        dataset = dataset.map(fix_explanations, desc='Fixing medmcqa explanations')
        return dataset


    @staticmethod
    def from_medqa(path: str, output_path: str = None, llm_output_column: str = 'output'):
        """
        Constructs the MedHal dataset from a processed MedQA dataset.

        Args:
            path: Path to the processed MedQA dataset.
            output_path: Path to the output file where the MedHal samples will be saved.
            llm_output_column: Column name of the LLM output in the MedQA dataset.
        """
        dataset = HuggingFaceDataset.from_csv(path)
        
        def filter_dataset(hf_dataset: HuggingFaceDataset) -> HuggingFaceDataset:
            chosen_indices = [] # row indices we will keep
            label_seen = defaultdict(set) # (idx, key_path) -> {labels kept}

            for i in tqdm(range(len(hf_dataset)), total=len(hf_dataset)):
                key = hf_dataset[i]['question'] + hf_dataset[i]['options']
                label = bool(hf_dataset[i]['is_correct'])
                if label not in label_seen[key]:
                    # First time we see this label
                    chosen_indices.append(i)
                    label_seen[key].add(label)
                
            # Only keep pairs that have both labels
            valid_keys = {k for k, labs in label_seen.items() if labs == {True, False}}
            final_indices = [i for i in chosen_indices
                            if hf_dataset[i]['question'] + hf_dataset[i]['options'] in valid_keys]

            return hf_dataset.select(sorted(final_indices))

        def transform_medqa_to_medhal(x):
            if not x['is_correct'][0] or x['is_correct'][1]:
                a = 2

            assert x['is_correct'][0] and not x['is_correct'][1], "The first sample must be correct and the second one must be incorrect"
            # assert int(x['id'][0]) == int(x['id'][1]) - 1, "The first sample must be before the second one"

            return {
                'id': [str(uuid.uuid4()) for _ in range(len(x['id']))],
                'context': x['context'],
                'statement': x[llm_output_column],
                'label': x['is_correct'],
                'explanation': [MedHal.FACTUAL_EXPLANATION, x[llm_output_column][0]],
                'inner_id': [str(id) for id in x['id']],
                'source': ['medqa'] * len(x['id']),
                'synthetic': [False] * len(x['id'])
            }

        dataset = filter_dataset(dataset)
        dataset = dataset.sort(['question', 'options'])
        print('Final length : ', len(dataset))

        dataset = dataset.map(
            transform_medqa_to_medhal, 
            desc="Transforming MedQA to MedHal",
            batched=True,
            batch_size=2,
            remove_columns=dataset.column_names
        )

        dataset = dataset.filter(MedHal.filter_multiple_sentence_statements, desc="Filtering multiple sentence statements")

        if output_path is not None:
            dataset.to_csv(output_path, index=False)

        return dataset

    @staticmethod
    def from_splitted_sumpubmed(positive_path: str, negative_path: str, output_path: str = None, llm_output_column: str = 'output'):
        """
        Constructs the MedHal dataset from a splitted SumPubMed dataset. The dataset is splitted in the sense
        that positive and negative samples are in different files.

        Args:
            positive_path: Path to the positive samples.
            negative_path: Path to the negative samples.
            output_path: Path to the output file where the MedHal samples will be saved.
            llm_output_column: Column name of the LLM output in the negative samples.
        """
        positive_dataset = HuggingFaceDataset.from_csv(positive_path)
        negative_dataset = HuggingFaceDataset.from_csv(negative_path)

        def generate_positive_samples(x):

            unique_ids = [str(uuid.uuid4()) for _ in range(len(x['text']))]

            return {
                'id': unique_ids,
                'context': x['text'],
                'statement': x['summary'],
                'label': [True] * len(x['text']),
                'explanation': [MedHal.FACTUAL_EXPLANATION] * len(x['text']),
                'inner_id': [str(id) for id in x['id']],
                'source': ['sumpubmed'] * len(x['text']),
                'synthetic': [False] * len(x['text'])
            }

        positive_ready_dataset = positive_dataset.map(
            generate_positive_samples, 
            remove_columns=positive_dataset.column_names,
            batched=True,
            desc="Generating positive samples"
        )

        def generate_negative_samples(x):
            unique_ids = [str(uuid.uuid4()) for _ in range(len(x['text']))]

            fake_summaries = []
            for before, output, after in zip(x['before'], x[llm_output_column], x['after']):
                fake_summaries.append(f"{before} {output.lower().strip()} {after}".strip())

            explanations = []
            for explanation in x['sentence']:
                explanations.append(f"According to the context, {explanation}")

            return {
                'id': unique_ids,
                'context': x['text'],
                'statement': fake_summaries,
                'label': [False] * len(x['text']),
                'explanation': explanations,
                'inner_id': [str(id) for id in x['id']],
                'source': ['sumpubmed'] * len(x['text']),
                'synthetic': [True] * len(x['text'])
            }
        
        negative_ready_dataset = negative_dataset.map(
            generate_negative_samples,
            remove_columns=negative_dataset.column_names,
            batched=True,
            desc="Generating negative samples"
        )

        # Merge the positive and negative datasets
        merged_dataset: HuggingFaceDataset = concatenate_datasets([positive_ready_dataset, negative_ready_dataset])

        if output_path is not None:
            merged_dataset.to_csv(output_path, index=False)

        return merged_dataset
